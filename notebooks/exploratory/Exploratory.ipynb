{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SalePrice</th>\n",
       "      <th>Township</th>\n",
       "      <th>SqFtLot</th>\n",
       "      <th>MtRainier</th>\n",
       "      <th>Olympics</th>\n",
       "      <th>Cascades</th>\n",
       "      <th>Territorial</th>\n",
       "      <th>SeattleSkyline</th>\n",
       "      <th>PugetSound</th>\n",
       "      <th>LakeWashington</th>\n",
       "      <th>...</th>\n",
       "      <th>YrBuilt</th>\n",
       "      <th>YrRenovated</th>\n",
       "      <th>PcntComplete</th>\n",
       "      <th>Condition</th>\n",
       "      <th>AddnlCost</th>\n",
       "      <th>SaleWarning</th>\n",
       "      <th>View_N</th>\n",
       "      <th>View_Y</th>\n",
       "      <th>TotBathrooms</th>\n",
       "      <th>TotFireplace</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>800000</td>\n",
       "      <td>26</td>\n",
       "      <td>10560</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1968</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>15 51</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3.25</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>730000</td>\n",
       "      <td>26</td>\n",
       "      <td>9853</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1969</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>875000</td>\n",
       "      <td>25</td>\n",
       "      <td>3600</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1919</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>249950</td>\n",
       "      <td>26</td>\n",
       "      <td>7750</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2019</td>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>3</td>\n",
       "      <td>5000</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.25</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>205000</td>\n",
       "      <td>26</td>\n",
       "      <td>7750</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2019</td>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>3</td>\n",
       "      <td>5000</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.25</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   SalePrice  Township  SqFtLot  MtRainier  Olympics  Cascades  Territorial  \\\n",
       "0     800000        26    10560          0         2         0            2   \n",
       "1     730000        26     9853          0         0         0            0   \n",
       "2     875000        25     3600          0         0         0            0   \n",
       "3     249950        26     7750          0         0         0            0   \n",
       "4     205000        26     7750          0         0         0            0   \n",
       "\n",
       "   SeattleSkyline  PugetSound  LakeWashington  ...  YrBuilt  YrRenovated  \\\n",
       "0               0           2               0  ...     1968            0   \n",
       "1               0           0               0  ...     1969            0   \n",
       "2               0           0               0  ...     1919            0   \n",
       "3               0           0               0  ...     2019            0   \n",
       "4               0           0               0  ...     2019            0   \n",
       "\n",
       "   PcntComplete  Condition  AddnlCost  SaleWarning  View_N  View_Y  \\\n",
       "0             0          4          0        15 51       1       0   \n",
       "1             0          3          0                    0       0   \n",
       "2             0          3          0                    0       0   \n",
       "3            58          3       5000           10       0       0   \n",
       "4            58          3       5000           15       0       0   \n",
       "\n",
       "   TotBathrooms  TotFireplace  \n",
       "0          3.25             2  \n",
       "1          3.00             2  \n",
       "2          1.00             1  \n",
       "3          3.25             2  \n",
       "4          3.25             2  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing pandas \n",
    "import pandas as pd\n",
    "\n",
    "# reading csv file\n",
    "df=pd.read_csv('../../data/processed/merged_data.csv')\n",
    "\n",
    "# previewing df\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.diagnostic import linear_rainbow, het_breuschpagan\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pickle\n",
    "\n",
    "# pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.max_rows', None)\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "sns.set_context(\"paper\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('../../data/processed/merged_data.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = np.abs(df.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = corr.sort_values('SalePrice', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Including mask for readibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.triu(np.ones_like(corr, dtype=np.bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating correlogram to display correlation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1, ax1 = plt.subplots(figsize=(12,10))\n",
    "sns.heatmap(corr, mask=mask, ax=ax1, cmap=\"tab20c\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1 - Correlated Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cor_cols = ['SqFtTotLiving', 'TotBathrooms','Bedrooms', 'TotFireplace', 'SqFtOpenPorch', 'SalePrice']\n",
    "cor_cols_df = df[cor_cols]\n",
    "sns.pairplot(cor_cols_df)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pairplot visualizes the correlation between the selected variables. While none of variables appear to have a strong linear relationship, SqFtTotLiving had the highest correlation and will be used for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsm_df = df[[\"SalePrice\", \"SqFtTotLiving\"]].copy()\n",
    "fsm_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsm = ols(formula=\"SalePrice ~ SqFtTotLiving\", data = fsm_df)\n",
    "fsm_results = fsm.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsm_results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1 Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R2 and Adj R2 are both 0.152 so only about 15% of the variability in SalePrice (dependent variable) can be  explained by SqFtTotLiving (independent variable) using this model.\n",
    "\n",
    "The Prob(F-statistic) which estimates the likelihood that this model results the way it does by chance is 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Linearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rainbow_statistic, rainbow_p_value = linear_rainbow(fsm_results)\n",
    "print(\"Rainbow statistic:\", rainbow_statistic)\n",
    "print(\"Rainbow p-value:\",rainbow_p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used the linear_rainbox(model_results) from Stats models. Its null-hypothesis is that the model shows linearity. The alternate hypothesis is that it does not. The p-value is low meaning that we have sufficient evidence to reject the null-hypothesis. The model violates the assumption of linearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Normality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this I used the Jarque-Bera test and Jarque-Bera (JB) p-value. The null hypothesis is that the residuals are normally distributed.  The alternative is that they are not. The p-value is 0 meaning that normality is violated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Homoscedasticity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homoscedasticity can be visualized using the predicted SalePrice vs the residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = fsm_df['SalePrice']\n",
    "y_hat = fsm_results.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2, ax1 = plt.subplots(figsize=(8,6))\n",
    "ax1.set_title(\"Model 1 Sale Price Prediction\")\n",
    "ax1.set(xlabel=\"Predicted Sale Price\", ylabel=\"Residuals (Predicted Sale Price - Actual)\")\n",
    "ax1.scatter(x=y_hat, y=y_hat-y, color='red', alpha=0.2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm, lm_p_value, fvalue, f_p_value = het_breuschpagan(y-y_hat, fsm_df[['SalePrice']])\n",
    "\n",
    "print(\"Lagrange Multiplier p-value:\", lm_p_value)\n",
    "print(\"F-Statistic p-value:\", f_p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The null hypothesis is that the data is homoscedastic. Based on the graphical representation and het_breuschpagan test, we have sufficient evidence to reject the null hypothesis. There appears to be an underestimation for SalePrice. Further investigation is needed for this dependent variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2 - Correlated Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Further investigating data for outliers and reasonability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig3, ([ax1, ax2], [ax3, ax4], [ax5, ax6]) = plt.subplots(3, 2, figsize=(10,5))\n",
    "\n",
    "sns.boxplot(x=df.Bedrooms, ax=ax1);\n",
    "sns.boxplot(x=df.TotBathrooms, ax=ax2);\n",
    "sns.boxplot(x=df.SqFtTotLiving, ax=ax3);\n",
    "sns.boxplot(x=df.SalePrice, ax=ax4);\n",
    "sns.boxplot(x=df.TotFireplace, ax=ax5);\n",
    "sns.boxplot(x=df.SqFtOpenPorch, ax=ax6);\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 1 calculated that a skew of as 10.995, a number that should be closer to zero for normally distributed data. The boxplots show that the data is heavily skewed to the right and is heavily impacted by outliers. Outliers will be removed. A log transformation will be used to normalize the data for use in model 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_normal = ['SalePrice','SqFtTotLiving', 'TotBathrooms','Bedrooms','SqFtOpenPorch', 'TotFireplace']\n",
    "\n",
    "Q1 = df[non_normal].quantile(0.25)\n",
    "Q3 = df[non_normal].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "df = df[~((df[non_normal] < (Q1 - 1.5 * IQR)) |\n",
    "          (df[non_normal] > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "len(df) #51160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig4, ([ax1, ax2], [ax3, ax4], [ax5, ax6]) = plt.subplots(3, 2, figsize=(10,5))\n",
    "\n",
    "sns.boxplot(x=df.Bedrooms, ax=ax1);\n",
    "sns.boxplot(x=df.TotBathrooms, ax=ax2);\n",
    "sns.boxplot(x=df.SqFtTotLiving, ax=ax3);\n",
    "sns.boxplot(x=df.SalePrice, ax=ax4);\n",
    "sns.boxplot(x=df.TotFireplace, ax=ax5);\n",
    "sns.boxplot(x=df.SqFtOpenPorch, ax=ax6);\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "non_normal2 = ['SalePrice','SqFtTotLiving', 'TotBathrooms','Bedrooms','SqFtOpenPorch', 'TotFireplace']\n",
    "\n",
    "for feat in non_normal:\n",
    "    df[feat] = df[feat].map(lambda x: np.sqrt(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor_cols2 = ['SqFtTotLiving', 'TotBathrooms','Bedrooms', 'TotFireplace', 'SqFtOpenPorch','SalePrice']\n",
    "cor_cols_df2 = df[cor_cols2]\n",
    "sns.pairplot(cor_cols_df2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pairplot visualizes the correlation between the selected variables. While none of variables appear to have a strong linear relationship, SqFtTotLiving has the highest correlation and will be used for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssm_df = df[[\"SalePrice\", \"SqFtTotLiving\", \"TotBathrooms\"]].copy()\n",
    "ssm_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssm = ols(formula=\"SalePrice ~ SqFtTotLiving + TotBathrooms\", data = ssm_df)\n",
    "ssm_results = ssm.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssm_results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2 Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R2 and Adj R2 are both 0.226 so only about 25% of the variability in SalePrice (dependent variable) can be explained by SqFtTotLiving and TotBathroom (independent variables) after the data was transformed. This is higher than model 1.\n",
    "\n",
    "The Prob(F-statistic) which estimates the likelylood that this model resulting the way it does by chance is still 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Linearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rainbow_statistic, rainbow_p_value = linear_rainbow(ssm_results)\n",
    "print(\"Rainbow statistic:\", rainbow_statistic)\n",
    "print(\"Rainbow p-value:\",rainbow_p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used the linear_rainbox(model_results) from Stats models. Its null-hypothesis is that the model shows linearity. The alternate hypothesis is that it does not. The p-value is low meaning that we have sufficient evidence to reject the null-hypothesis. The model violates the assumption of linearity. I must also note, that this result is higher than that of model 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Normality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this I used the Jarque-Bera test and Jarque-Bera (JB) p-value. The null hypothesis is that the residuals are normally distributed. The p-value is still at 0 meaning that the normality assumption is still violated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Homoscedasticity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be visualized using the predicted SalePrice vs the residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2 = ssm_df['SalePrice']\n",
    "y_hat2 = ssm_results.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig5, ax1 = plt.subplots(figsize=(8,6))\n",
    "ax1.set_title(\"Model 2 Sale Price Prediction\")\n",
    "ax1.set(xlabel=\"Predicted Sale Price\", ylabel=\"Residuals (Predicted Sale Price - Actual)\")\n",
    "ax1.scatter(x=y_hat2, y=y_hat2-y2, color='orange', alpha=0.2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm, lm_p_value, fvalue, f_p_value = het_breuschpagan(y2-y_hat2, ssm_df[['SalePrice']])\n",
    "\n",
    "print(\"Lagrange Multiplier p-value:\", lm_p_value)\n",
    "print(\"F-Statistic p-value:\", f_p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The null hypothesis is that the data is homoscedastic. Based on the graphical representation and het_breuschpagan test, we have sufficient evidence to reject the null hypothesis. While outliers were removed, it appears to be both an underestimation and overestimation for SalePrice. Further investigation is needed for this dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Independence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be tested using the variance inflation factor from Statsmodels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = ssm_df[[\"SqFtTotLiving\", \"TotBathrooms\"]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vif_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vif_df['VIF'] = [variance_inflation_factor(rows, i) for i in range(2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vif_df['Feature'] = [\"SqFtTotLiving\", \"TotBathrooms\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vif_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A VIF score of 5 is too high. So it is reasonable to say that we are violating the independence assumption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3 - Adding a Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tsm_df = df[[\"SalePrice\", \"SqFtTotLiving\", \"TotBathrooms\", \"Bedrooms\", \"View_Y\" ]].copy()\n",
    "tsm_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsm_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsm = ols(formula=\"SalePrice ~ SqFtTotLiving + TotBathrooms + Bedrooms + View_Y\", data = tsm_df)\n",
    "tsm_results = tsm.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsm_results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 3 Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R2 and Adj R2 are both .256 so about 26% of the variability in SalePrice (dependent variable) can be  explained by the independent variables in this model.\n",
    "\n",
    "The Prob(F-statistic) which estimates the likelihood that this model results the way it does is 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Linearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rainbow_statistic, rainbow_p_value = linear_rainbow(tsm_results)\n",
    "print(\"Rainbow statistic:\", rainbow_statistic)\n",
    "print(\"Rainbow p-value:\",rainbow_p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used the linear_rainbox(model_results) from Stats models. Its null-hypothesis is that the model shows linearity. The alternate hypothesis is that it does not. The p-value is low meaning that we have sufficient evidence to reject the null-hypothesis. The model violates the assumption of linearity. I must note that this result is higher than that of model 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Normality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this I used the Jarque-Bera test and Jarque-Bera (JB) p-value. The null hypothesis is that the residuals are normally distributed.  The alternative is that they are not. The p-value is 0 meaning that normality is violated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Homoscedasticity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be visualized using the predicted SalePrice vs the residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y3 = tsm_df['SalePrice']\n",
    "y_hat3 = tsm_results.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig8, ax1 = plt.subplots(figsize=(8,6))\n",
    "ax1.set_title(\"Model 3 Sale Price Prediction\")\n",
    "ax1.set(xlabel=\"Predicted Sale Price\",\n",
    "        ylabel=\"Residuals (Predicted Sale Price - Actual)\")\n",
    "ax1.scatter(x=y_hat3, y=y_hat3-y3, color='green', alpha=0.2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm, lm_p_value, fvalue, f_p_value = het_breuschpagan(y3-y_hat3, tsm_df[['SalePrice']])\n",
    "\n",
    "print(\"Lagrange Multiplier p-value:\", lm_p_value)\n",
    "print(\"F-Statistic p-value:\", f_p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the graphical representation and het_breuschpagan test, we have sufficient evidence to reject the null hypothesis. There appears to be both an underestimation and overestimation for SalePrice. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "I started with a baseline model where the only input feature was `SqFtTotLiving`.  The baseline model had an r-squared of 0.152.  This model violated the linearity (p < 0.001), normality (p < 0.001), and homoscedasticity (p < 0.001) assumptions of linear regression.  The independence assumption was met by default because there was only one input feature.\n",
    "\n",
    "The second model used `TotBathrooms` as the other input feature.  The baseline model had an r-squared of 0.226.  This model violated the linearity (p < 0.001), normality (p < 0.001), and homoscedasticity (p < 0.001) assumptions of linear regression.  The independence assumption was not met.\n",
    "\n",
    "The final model added a categorical feature `ViewUtilization`.  It had an r-squared of 0.256.  This model violated the linearity (p < 0.001), normality (p < 0.001), and homoscedasticity (p < 0.001) assumptions of linear regression.  The independence assumption was not met.\n",
    "\n",
    "\n",
    "The data analysis supports the following recommendations for home owners hoping to increase the sale price of their home.\n",
    "\n",
    "#### 1. Have a least 2 , 4-5 bedrooms ####\n",
    "\n",
    "The model used in this analysis showed the coefficient for bedrooms to be -99.2213. For each increase of bedrooms by 1 unit, it is predicted to have a -99.22 on the Sale Price. Using a bar graph to map this data it was revealed that homes with three bedrooms sold for less than homes with 2 or 4-5 bedrooms.\n",
    "\n",
    "#### 2. Increase total bathrooms to 2.25-3.75 ####\n",
    "\n",
    "The model used in this analysis showed the coefficient for the  bathrooms to be 11.7288. This means, for each increase by 1 unit of bathrooms, it is predicted to have a change on 11.79 on the Sale Price.\n",
    "\n",
    "#### 3. Utilize surrounding view, if available. #### \n",
    "127.0248\n",
    "The highest positive coefficient predicted by the model was the utilizing the available view. The coefficient for Utilizing view feature was 127.02. Because the data here was categorical and binary, one unit (or actually utilizing the view) was predicted to have a positive change of 127.02 on Sale Price.\n",
    "\n",
    "#### 4. Consider projects that will expand the total living space. #### \n",
    "The model used in this analysis showed the coefficient for the  total living space to be 15.1737. This means, for each increase by 1 unit of total living, it is predicted to have a change on 15.17 on the Sale Price.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
